<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习,">










<meta name="description" content="引言隐马尔可夫模型是关于时许序列的概率模型。是由一个隐藏的马尔科夫链随机生成不可观测的状态序列，然后由状态序列生成一个观测序列的过程。 每个状态生成一个观测，同时每个状态会转移到下一个状态。因此马尔科夫过程有两个关键的序列，一个是状态序列，一个是观测序列。  上图就是一个标准的马尔科夫过程的观测输出以及状态转移过程。 隐马尔科夫模型由初始概率分布，转移状态矩阵概率分布，观测状态概率矩阵确定。 假设">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="隐马尔科夫模型">
<meta property="og:url" content="http://yoursite.com/2018/07/10/MachineLearning/隐马尔科夫模型/index.html">
<meta property="og:site_name" content="axis tech zone">
<meta property="og:description" content="引言隐马尔可夫模型是关于时许序列的概率模型。是由一个隐藏的马尔科夫链随机生成不可观测的状态序列，然后由状态序列生成一个观测序列的过程。 每个状态生成一个观测，同时每个状态会转移到下一个状态。因此马尔科夫过程有两个关键的序列，一个是状态序列，一个是观测序列。  上图就是一个标准的马尔科夫过程的观测输出以及状态转移过程。 隐马尔科夫模型由初始概率分布，转移状态矩阵概率分布，观测状态概率矩阵确定。 假设">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/images/markov.jpg">
<meta property="og:image" content="http://yoursite.com/images/houxiang.jpg">
<meta property="og:updated_time" content="2021-06-23T15:32:54.666Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="隐马尔科夫模型">
<meta name="twitter:description" content="引言隐马尔可夫模型是关于时许序列的概率模型。是由一个隐藏的马尔科夫链随机生成不可观测的状态序列，然后由状态序列生成一个观测序列的过程。 每个状态生成一个观测，同时每个状态会转移到下一个状态。因此马尔科夫过程有两个关键的序列，一个是状态序列，一个是观测序列。  上图就是一个标准的马尔科夫过程的观测输出以及状态转移过程。 隐马尔科夫模型由初始概率分布，转移状态矩阵概率分布，观测状态概率矩阵确定。 假设">
<meta name="twitter:image" content="http://yoursite.com/images/markov.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/07/10/MachineLearning/隐马尔科夫模型/">





  <title>隐马尔科夫模型 | axis tech zone</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">axis tech zone</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">a personal tech blog website</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/10/MachineLearning/隐马尔科夫模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="changyuan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="axis tech zone">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">隐马尔科夫模型</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-10T00:00:00+08:00">
                2018-07-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/10/MachineLearning/隐马尔科夫模型/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count gitment-comments-count" data-xid="/2018/07/10/MachineLearning/隐马尔科夫模型/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1,794
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  6
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>隐马尔可夫模型是关于时许序列的概率模型。是由一个隐藏的马尔科夫链随机生成不可观测的状态序列，然后由状态序列生成一个观测序列的过程。</p>
<p>每个状态生成一个观测，同时每个状态会转移到下一个状态。因此马尔科夫过程有两个关键的序列，一个是状态序列，一个是观测序列。</p>
<p><img src="/images/markov.jpg" alt="markov"></p>
<p>上图就是一个标准的马尔科夫过程的观测输出以及状态转移过程。</p>
<p>隐马尔科夫模型由初始概率分布，转移状态矩阵概率分布，观测状态概率矩阵确定。</p>
<p>假设我们的状态集合为<script type="math/tex">Q = \left \{ q_1,...,q_N \right\}</script></p>
<p>观测集合为<script type="math/tex">V = \left \{ v_1,...,v_M \right\}</script></p>
<p>上面的两个集合是观测和状态的取值集合。</p>
<p>假设我们的markov过程的状态序列为<script type="math/tex">I = (i_1,...,i_T)</script></p>
<p>观测序列为<script type="math/tex">O = ( o_1,...,o_T)</script></p>
<p>我们可以写出状态的转移矩阵<script type="math/tex">A = [a_{ij}]_{N*N}</script>,其中<script type="math/tex">a_{ij} = P(i_{t+1} = q_j | i_{t} = q_i )</script>,</p>
<p>也就是从<script type="math/tex">t</script>时间点状态<script type="math/tex">i</script>转移到<script type="math/tex">t+1</script>时间点状态<script type="math/tex">j</script>的概率。</p>
<p>我们也可以写出观测矩阵<script type="math/tex">B = [b_{j}(k)]_{N*M}</script>,其中<script type="math/tex">b_{j}(k) = P(o_{t} = v_k | i_{t} = q_j )</script>,</p>
<p>也就是从<script type="math/tex">t</script>时间点状态<script type="math/tex">j</script>的观测值为<script type="math/tex">v_k</script>的概率。</p>
<p>再加上我们的初始概率<script type="math/tex">\pi = (\pi_i)</script>,其中<script type="math/tex">\pi_i = P(i_1=q_i)</script></p>
<p>因此本质上我们的markov过程受如下的参数控制：</p>
<script type="math/tex; mode=display">\lambda = (\pi,A,B)</script><h1 id="隐markov过程需要满足的假设条件"><a href="#隐markov过程需要满足的假设条件" class="headerlink" title="隐markov过程需要满足的假设条件"></a>隐markov过程需要满足的假设条件</h1><p>1） 齐次马尔科夫假设</p>
<p>即<script type="math/tex">P(i_t | i_{t-1},o_{t-1},...,i_1,o_1) = P(i_t | i_{t-1})</script></p>
<p>也就是<script type="math/tex">t</script>时刻的状态只决定于<script type="math/tex">t-1</script>时刻的状态，跟其他时刻的状态以及观测无关。</p>
<p>2） 观测独立性假设</p>
<p>即<script type="math/tex">P(o_t | i_T,o_T,i_{T-1},o_{T-1},...,i_1,o_1) = P(o_t | i_{t})</script></p>
<p>也就是某一时刻的观测只与这一时刻的状态有关，跟其他时刻的状态以及观测无关。</p>
<p>不满足这两个朴素的假设条件，后面的一切推导都是错误的。</p>
<h1 id="隐马尔可夫模型的三个基础问题"><a href="#隐马尔可夫模型的三个基础问题" class="headerlink" title="隐马尔可夫模型的三个基础问题"></a>隐马尔可夫模型的三个基础问题</h1><p>1） 概率计算</p>
<p>即给定参数<script type="math/tex">\lambda = (\pi,A,B)</script>和观测<script type="math/tex">O = \left \{ o_1,...,o_T \right\}</script>，计算<script type="math/tex">P(O|\lambda)</script>，即某一观测序列出现的概率问题。</p>
<p>2） 学习问题</p>
<p>即已知<script type="math/tex">O = \left \{ o_1,...,o_T \right\}</script>，估计参数<script type="math/tex">\lambda = (\pi,A,B)</script>，即参数估计问题。</p>
<p>3） 预测问题</p>
<p>已知<script type="math/tex">\lambda = (\pi,A,B)</script>和观测<script type="math/tex">O = \left \{ o_1,...,o_T \right\}</script>，求使<script type="math/tex">P(I|O)</script>最大的状态序列，也就是能得到当前观测的最可能的状态序列。</p>
<p>下面我们就这三个问题，一个一个的解决</p>
<h2 id="概率计算问题"><a href="#概率计算问题" class="headerlink" title="概率计算问题"></a>概率计算问题</h2><p>概率计算就是计算<script type="math/tex">P(O|\lambda)</script>，给定了<script type="math/tex">\lambda = (\pi,A,B)</script>，这个计算就是一个简单的算数问题。那么我们来看下直接计算：</p>
<h3 id="直接计算"><a href="#直接计算" class="headerlink" title="直接计算"></a>直接计算</h3><p>首先第一步，计算状态序列为<script type="math/tex">I = ( i_1,...,i_T)</script>时的概率为：</p>
<script type="math/tex; mode=display">P(I|\lambda) = \pi_{i_1}a_{i_1i_2}a_{i_2i_3}...a_{i_{T-1}i_T}</script><p>当状态序列为<script type="math/tex">I = ( i_1,...,i_T)</script>时，观测序列为<script type="math/tex">O = ( o_1,...,o_T)</script>时的概率为：</p>
<script type="math/tex; mode=display">P(O | I, \lambda) = b_{i_1}(o_1) b_{i_2}(o_2) ... b_{i_T}(o_T)</script><p>根据条件概率，可以得到：</p>
<script type="math/tex; mode=display">P(O, I | \lambda) = \pi_{i_1} b_{i_1}(o_1) a_{i_1i_2} b_{i_2}(o_2) ... a_{i_{T-1}i_T} b_{i_T}(o_T)</script><p>对所有可能的状态求和，可以得到：</p>
<script type="math/tex; mode=display">P(O | \lambda) = \sum_I P(O, I | \lambda) = \sum_{i_{1},...,i_{T}} \pi_{i_1} b_{i_1}(o_1) a_{i_1i_2} b_{i_2}(o_2) ... a_{i_{T-1}i_T} b_{i_T}(o_T)</script><p>这样计算是可以的，但是这个计算量太大了，是<script type="math/tex">O(TN^T)</script>阶的。</p>
<p>因此引入了前向算法和后向算法。这两个算法能有效的降低计算复杂度。</p>
<h3 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h3><p>首先定义前向概率：</p>
<script type="math/tex; mode=display">\alpha_t(i) = P(o_1,...,o_t, i_t = q_i|\lambda)</script><p>前向概率表示时刻<script type="math/tex">t</script>，部分观测序列为<script type="math/tex">o_1,...,o_t</script>, 且时刻<script type="math/tex">t</script>状态为<script type="math/tex">q_i</script>的概率</p>
<p>前向算法：</p>
<p>初始值：<script type="math/tex">\alpha_1(i) = P(o_1, i_1 = q_i|\lambda) = \pi_ib_i(o_1)</script></p>
<p>递推：<script type="math/tex">\alpha_{t+1}(i) = P(o_1,...,o_t,o_{t+1}, i_{t+1} = q_i|\lambda) = [\sum_{j=1}^N \alpha_t(j)a_{ji}] b_i(o_{t+1})</script></p>
<p>终止：<script type="math/tex">\alpha_T(i) = P(o_1,...,o_T, i_T = q_i|\lambda)</script></p>
<script type="math/tex; mode=display">P(O|\lambda) = \sum_{i=1}^N \alpha_T(i)</script><p>因此前向算法的本质就是计算前向概率，然后将其递推到全局</p>
<h3 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h3><p>首先定义后向概率</p>
<script type="math/tex; mode=display">\beta_t(i) = P(o_{t+1},...,o_T | i_t = q_i, \lambda)</script><p>后向算法：</p>
<p>初始： <script type="math/tex">\beta_T(i) = 1</script></p>
<p>递推： 对于<script type="math/tex">t = T-1, T-2,...,1</script></p>
<script type="math/tex; mode=display">\beta_t(i) = P(o_{t+1},...,o_T | i_t = q_i, \lambda) = \sum_{j=1}^N a_{ij} b_j(o_{t+1})\beta_{t+1}(j)</script><p>终止：<script type="math/tex">P(O|\lambda) = \sum_{i=1}^N \pi_i b_i(o_1)\beta_1(i)</script></p>
<p>后向算法的递推图示：</p>
<p><img src="/images/houxiang.jpg" alt="DecisionTree"></p>
<h3 id="前向后向概率表示"><a href="#前向后向概率表示" class="headerlink" title="前向后向概率表示"></a>前向后向概率表示</h3><p>利用前向后向概率可以得到：</p>
<script type="math/tex; mode=display">P(O|\lambda) = \sum_{i=1}^N \sum_{j=1}^N \alpha_t(i) a_{ij} b_{j}(o_{t+1})\beta_{t+1}(j)</script><p>推导如下：</p>
<script type="math/tex; mode=display">\alpha_t(i) = P(o_1,...,o_t, i_t = q_i|\lambda)</script><script type="math/tex; mode=display">\beta_t(i) = P(o_{t+1},...,o_T | i_t = q_i, \lambda)</script><p>则<script type="math/tex">\alpha_t(i) a_{ij}</script>表示观测为<script type="math/tex">o_1,...,o_t</script>，状态由<script type="math/tex">q_i</script>转到<script type="math/tex">q_j</script>的概率</p>
<p>则<script type="math/tex">\alpha_t(i) a_{ij} b_j(o_{t+1})</script>表示观测为<script type="math/tex">o_1,...,o_t,o_{t+1}</script>，状态由<script type="math/tex">q_i</script>转到<script type="math/tex">q_j</script>的概率</p>
<p>则则<script type="math/tex">\alpha_t(i) a_{ij} b_j(o_{t+1}) \beta_{t+1}(j)</script> 表示观测为<script type="math/tex">o_1,...,o_t,...,o_{T}</script>，状态由<script type="math/tex">q_i</script>转到<script type="math/tex">q_j</script>的概率</p>
<p>对状态<script type="math/tex">i</script>和状态<script type="math/tex">j</script>求和，就可以得到<script type="math/tex">P(O|\lambda)</script></p>
<h3 id="前向后向概率的几个特殊的统计值"><a href="#前向后向概率的几个特殊的统计值" class="headerlink" title="前向后向概率的几个特殊的统计值"></a>前向后向概率的几个特殊的统计值</h3><ol>
<li>定义给定参数<script type="math/tex">O, \lambda</script>的情况下，时刻<script type="math/tex">t</script>处于状态<script type="math/tex">q_i</script>的概率为:</li>
</ol>
<script type="math/tex; mode=display">\gamma_t(i) =P(i_t = q_i|O, \lambda)</script><p>则：</p>
<script type="math/tex; mode=display">\gamma_t(i) =P(i_t = q_i|O, \lambda) = \frac{P(i_t = q_i, O| \lambda)}{P(O|\lambda)}</script><p>由前向概率和后向概率</p>
<script type="math/tex; mode=display">\alpha_t(i) = P(o_1,...,o_t, i_t = q_i|\lambda)</script><script type="math/tex; mode=display">\beta_t(i) = P(o_{t+1},...,o_T | i_t = q_i, \lambda)</script><p>可以得到：</p>
<script type="math/tex; mode=display">P(i_t = q_i, O| \lambda) = P(o_1,...,o_t,...,o_T, i_t = q_i|\lambda)</script><script type="math/tex; mode=display">= P(o_1,...,o_t,...,o_T| i_t = q_i, \lambda) P( i_t = q_i| \lambda)</script><script type="math/tex; mode=display">= P(o_1,...,o_t| i_t = q_i, \lambda) P(o_{t+1},...,o_T|  i_t = q_i, \lambda) P( i_t = q_i| \lambda)</script><script type="math/tex; mode=display">= P(o_1,...,o_t| i_t = q_i, \lambda)P( i_t = q_i| \lambda) P(o_{t+1},...,o_T|  i_t = q_i, \lambda)</script><script type="math/tex; mode=display">= P(o_1,...,o_t, i_t = q_i|\lambda)  P(o_{t+1},...,o_T|  i_t = q_i, \lambda)</script><script type="math/tex; mode=display">= \alpha_t(i) \beta_t(i)</script><p>于是可以得到：</p>
<script type="math/tex; mode=display">\gamma_t(i) =P(i_t = q_i|O, \lambda) = \frac{\alpha_t(i) \beta_t(i)}{\sum_j^N \alpha_t(j) \beta_t(j)}</script><ol>
<li><p>定义给定参数<script type="math/tex">O, \lambda</script>的情况下，时刻<script type="math/tex">t</script>处于状态<script type="math/tex">q_i</script>且时刻<script type="math/tex">t+1</script>处于状态<script type="math/tex">q_j</script>的概率为:</p>
<script type="math/tex; mode=display">\xi_t(i, j) = \frac{P(i_t = q_i,i_{t+1} = q_j, O|\lambda)}{P(O|\lambda)}</script></li>
</ol>
<p>而：</p>
<script type="math/tex; mode=display">P(i_t = q_i,i_{t+1} = q_j, O|\lambda)</script><script type="math/tex; mode=display">= P(o_1,...,o_t,...,o_T| i_t = q_i, i_{t+1} = q_j, \lambda) P( i_t = q_i, i_{t+1} = q_j|\lambda)</script><script type="math/tex; mode=display">= P(o_1,...,o_t,...,o_T| i_t = q_i, i_{t+1} = q_j, \lambda) P(i_{t+1} = q_j|i_t = q_i, \lambda) P(i_{t} = q_i| \lambda)</script><script type="math/tex; mode=display">= P(o_1,...,o_t| i_t = q_i,i_{t+1} = q_j, \lambda) P(o_{t+1}|  i_t = q_i, i_{t+1} = q_j,\lambda) P(o_{t+2},...,o_T|  i_t = q_i, i_{t+1} = q_j,\lambda) P(i_{t+1} = q_j|i_t = q_i, \lambda) P(i_{t} = q_i| \lambda)</script><script type="math/tex; mode=display">= P(o_1,...,o_t| i_t = q_i,i_{t+1} = q_j, \lambda) P(o_{t+1}|  i_t = q_i, i_{t+1} = q_j,\lambda) P(o_{t+2},...,o_T|  i_t = q_i, i_{t+1} = q_j,\lambda) a_{ij}P(i_{t} = q_i| \lambda)</script><script type="math/tex; mode=display">= P(o_1,...,o_t| i_t = q_i, i_{t+1} = q_j, \lambda)P(i_{t} = q_i| \lambda) P(o_{t+1}|  i_t = q_i, i_{t+1} = q_j,\lambda) P(o_{t+2},...,o_T|  i_t = q_i, i_{t+1} = q_j,\lambda) a_{ij}</script><script type="math/tex; mode=display">= P(o_1,...,o_t| i_t = q_i, i_{t+1} = q_j, \lambda)P(i_{t} = q_i| i_{t+1} = q_j, \lambda) P(o_{t+1}|  i_t = q_i, i_{t+1} = q_j,\lambda) P(o_{t+2},...,o_T|  i_t = q_i, i_{t+1} = q_j,\lambda) a_{ij}</script><script type="math/tex; mode=display">= P(o_1,...,o_t, i_t = q_i| i_{t+1} = q_j, \lambda) P(o_{t+1}|  i_t = q_i, i_{t+1} = q_j,\lambda) P(o_{t+2},...,o_T|  i_t = q_i, i_{t+1} = q_j,\lambda) a_{ij}</script><script type="math/tex; mode=display">= P(o_1,...,o_t, i_t = q_i| \lambda) P(o_{t+1}|  i_t = q_i,\lambda) P(o_{t+2},...,o_T| i_{t+1} = q_j,\lambda) a_{ij}</script><script type="math/tex; mode=display">= \alpha_t(i)b_j(o_{t+1}) \beta_t(j) a_{ij} = \alpha_t(i)a_{ij}b_j(o_{t+1}) \beta_t(j)</script><p>所以有：</p>
<script type="math/tex; mode=display">\xi_t(i, j) = \frac{P(i_t = q_i,i_{t+1} = q_j, O|\lambda)}{P(O|\lambda)} = \frac{\alpha_t(i)a_{ij}b_j(o_{t+1}) \beta_t(j) }{\sum_{i=1}^N \sum_{j=1}^N \alpha_t(i)a_{ij}b_j(o_{t+1}) \beta_t(j)}</script><p>这里我们之所以这么费事的要推导出这两个概率，是为了后面的预测算法做铺垫，后面会用到</p>
<h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p>上面我们推导了概率计算问题，那么下面我们就要来面对一个很机器学习的问题了，参数估计。给定数据，估计分布，这个是统计机器学习的主要内容。</p>
<p>首先如果我们已经知道了状态序列，那么这个参数估计就好办多了，直接统计值就行了，这种属于监督学习。</p>
<p>如果已知状态序列和观测序列为<script type="math/tex">O = \left \{ (o_1,I_i),...,(o_T,I_T) \right\}</script></p>
<p>则状态转移概率的估计：</p>
<script type="math/tex; mode=display">\hat{a}_{ij} = \frac{A_{ij}}{\sum_{j=1}^N A_{ij}}</script><p>其中<script type="math/tex">A_{ij}</script>为时刻<script type="math/tex">t</script>处于状态<script type="math/tex">i</script>且时刻<script type="math/tex">t+1</script>转移到状态<script type="math/tex">j</script>的频数</p>
<p>则观测概率的估计：</p>
<script type="math/tex; mode=display">\hat{b}_{j}(k) = \frac{B_{jk}}{\sum_{k=1}^M B_{jk}}</script><p>其中<script type="math/tex">B_{jk}</script>为状态<script type="math/tex">j</script>观测为<script type="math/tex">K</script>的频数。</p>
<p><strong><em>可是现实生活中哪有这样好的事情，我们最长面对的情况是只拿得到观测，而拿不到状态，这时，我们看怎样处理</em></strong></p>
<h3 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h3><p>如果已知观测序列为<script type="math/tex">O = \left \{ o_1,...,o_T \right\}</script>，状态序列未知，估计参数。我们发现这正好是EM算法的能力啊，只要将状态<script type="math/tex">I</script>作为隐变量，就可以用EM算法来解决这个问题了。</p>
<p>首先，我们知道</p>
<script type="math/tex; mode=display">P(O|\lambda) = \sum_I P(O,I|\lambda) = \sum_I P(O|I,\lambda)P(I|\lambda)</script><p>根据EM算法的步骤，首先我们需要写出完全数据的对数似然函数<script type="math/tex">logP(O,I|\lambda)</script>：</p>
<script type="math/tex; mode=display">Q(\lambda, \bar{\lambda}) = E_I[logP(O,I|\lambda)|O,\bar{\lambda}] = \sum_I logP(O,I|\lambda)P(I|O,\bar{\lambda}) = \frac{\sum_I logP(O,I|\lambda)P(I,O|\bar{\lambda})}{P(O|\bar{\lambda})}</script><p>其中<script type="math/tex">{P(O|\bar{\lambda})}</script>跟最优化<script type="math/tex">\lambda</script>无关，因此可以省略掉，因此完全函数可以写为：</p>
<script type="math/tex; mode=display">Q(\lambda, \bar{\lambda}) = \sum_I logP(O,I|\lambda)P(I,O|\bar{\lambda})</script><p>而：</p>
<script type="math/tex; mode=display">P(I,O|\bar{\lambda}) = \pi_{i_1} b_{i_1}(o_1) a_{i_1i_2} b_{i_2}(o_2) ... a_{i_{T-1}i_T} b_{i_T}(o_T)</script><p>因此<script type="math/tex">Q(\lambda, \bar{\lambda})</script>函数可以写为：</p>
<script type="math/tex; mode=display">Q(\lambda, \bar{\lambda}) = \sum_I log \pi_{i_1} P(I,O|\bar{\lambda}) + \sum_I(\sum_{t=1}^{T-1} log a_{i_t i_{t+1}}) P(O,I|\bar{\lambda}) + \sum_I(\sum_{t=1}^{T} log b_{i_t}(o_t)) P(O,I|\bar{\lambda})</script><p>E步完成后，就该M步的了：<br>由于我们需要极大话的三个项，分别在上面的三个分式中，因此单独极大化上面的分式即可。</p>
<p>第一项：</p>
<script type="math/tex; mode=display">\sum_I log \pi_{i_1} P(I,O|\bar{\lambda}) = \sum_1^N log\pi_iP(O,i_i=i|\bar(\lambda))</script><p>参数<script type="math/tex">\pi_i</script>满足的约束条件为<script type="math/tex">\sum_i^N \pi_i = 1</script>,因此利用拉格朗日乘子法，可以得到拉格朗日函数</p>
<script type="math/tex; mode=display">\sum_1^N log\pi_iP(O,i_i=i|\bar{\lambda)} + \gamma(\sum_i^N \pi_i - 1)</script><p>对其求偏导并令其结果为0，可以得到：</p>
<script type="math/tex; mode=display">\frac{\partial }{\partial \pi_i} [\sum_1^N log\pi_iP(O,i_i=i|\bar{\lambda)} + \gamma(\sum_i^N \pi_i - 1)] = 0</script><p>得到：</p>
<script type="math/tex; mode=display">P(O,i_i=i|\bar{\lambda}) + \gamma \pi_i =0</script><p>对i求和，可得：</p>
<script type="math/tex; mode=display">\gamma  = - P(O|\bar{\lambda})</script><p>带如到上式有：</p>
<script type="math/tex; mode=display">\pi_i  = \frac {P(O, i_1 = i|\bar{\lambda})}{P(O|\bar{\lambda})}</script><p>同理，可以得到另外两项的估计，如下所示：</p>
<script type="math/tex; mode=display">a_{ij}  = \frac {\sum_{t=1}^{T-1} P(O, i_t = i, i_{t+1} = j |\bar{\lambda})}{ \sum_{t=1}^{T-1} P(O, i_t =i|\bar{\lambda})}</script><script type="math/tex; mode=display">b_{j}(k)  = \frac {\sum_{t=1}^{T} P(O, i_t = j |\bar{\lambda})I(o_t = v_k)}{ \sum_{t=1}^{T} P(O, i_t =j|\bar{\lambda})}</script><p>如果引入我们之前的统计量</p>
<script type="math/tex; mode=display">\gamma_t(i) =P(i_t = q_i|O, \lambda) = \frac{P(i_t = q_i, O| \lambda)}{P(O|\lambda)}</script><script type="math/tex; mode=display">\xi_t(i, j) = \frac{P(i_t = q_i,i_{t+1} = q_j, O|\lambda)}{P(O|\lambda)}</script><p> 那么参数估计可以写为：</p>
<script type="math/tex; mode=display">a_{ij}  = \frac {\sum_{t=1}^{T-1} \xi_t(i, j)}{ \sum_{t=1}^{T-1} \gamma_t(i)}</script><script type="math/tex; mode=display">b_{j}(k)  = \frac {\sum_{t=1,o_t = v_k}^{T} \gamma_t(j)}{ \sum_{t=1}^{T} \gamma_t(j)}</script><script type="math/tex; mode=display">\pi_i  = \gamma_1(i)</script><h2 id="预测问题"><a href="#预测问题" class="headerlink" title="预测问题"></a>预测问题</h2><h3 id="近似算法"><a href="#近似算法" class="headerlink" title="近似算法"></a>近似算法</h3><p>所谓近似算法，就是每个时刻选择最优可能出现的状态，将她作为这个时刻的结果。</p>
<p>给定<script type="math/tex">\lambda, O</script>，有</p>
<script type="math/tex; mode=display">\gamma_t(i) = P(i_t=q_i|O,\lambda)  = \frac{P(i_t = q_i, O| \lambda)}{P(O|\lambda)} = \frac{\alpha_t(i) \beta_t(i)}{\sum_j^N \alpha_t(j) \beta_t(j)}</script><p>则，每个时刻的最优可能的状态：</p>
<script type="math/tex; mode=display">i_t^{*} = arg max_{1 \leq i \leq N} [\gamma_t(i)], t=1,2,..,T</script><p>进而得到状态序列<script type="math/tex">I^* = (i_1^*,...,i_T^{*})</script></p>
<p>近似算法简单，易于计算，但是这个不是最优的结果。其实我们的预测算法本质上就是一个最优路径的问题，因此可以引入动态规划的原理来解决，这就是Viterbi算法</p>
<h3 id="Viterbi算法"><a href="#Viterbi算法" class="headerlink" title="Viterbi算法"></a>Viterbi算法</h3><blockquote>
<p>输入: <script type="math/tex">\lambda=(A,B,\pi)</script>和<script type="math/tex">O=(o_1,...,o_T)</script></p>
<p>初始化：</p>
<blockquote>
<p>计算<script type="math/tex">\delta_1(i) = \pi_i b_i(o_1), i=1,2,...,N</script></p>
<script type="math/tex; mode=display">\psi_1(i) = 0</script></blockquote>
<p>更新参数</p>
<blockquote>
<script type="math/tex; mode=display">\delta_t(i) = max_{1 \leq j \leq N}[\delta_{t-1}(j)a_{ji}]b_i(o_t), i=1,2,...,N</script><script type="math/tex; mode=display">\psi_t(i) = max_{1 \leq j \leq N}[\delta_{t-1}(j)a_{ji}], i=1,2,...,N</script></blockquote>
<p>终止</p>
<blockquote>
<script type="math/tex; mode=display">P^* = max_{1 \leq i \leq N} (\delta_T(i))</script><script type="math/tex; mode=display">i_T^* = arg max_{1 \leq i \leq N} (\delta_T(i))</script></blockquote>
<p>最优路径回溯</p>
<blockquote>
<p>对<script type="math/tex">t =T-1,T-2,...,1</script></p>
<script type="math/tex; mode=display">i_t^* = \psi_{t+1} (i_{t+1}^*)</script></blockquote>
</blockquote>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>上面就是隐马尔可夫模型的三个基本问题以及相应的求解方式。本质上讲，隐马尔可夫模型比较简单，但是也比较常用，在分词，语音识别方面有着广泛的应用，不过由于深度学习横空出世，单纯的隐马尔科夫模型略显无奈啊。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/01/DataStructure/回溯算法/" rel="next" title="回溯算法">
                <i class="fa fa-chevron-left"></i> 回溯算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/15/ProgramingLanguage/重构书中的例子总结/" rel="prev" title="重构书中例子的总结">
                重构书中例子的总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div onclick="showGitment()" id="gitment-display-button">显示 Gitment 评论</div>
        <div id="gitment-container" style="display:none"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="changyuan">
            
              <p class="site-author-name" itemprop="name">changyuan</p>
              <p class="site-description motion-element" itemprop="description">所谓妖，只不过是求而不得的人，修而未成的果。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">184</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">50</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/changyuanchn" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://blog.csdn.net/changyuanchn" target="_blank" title="CSDN">
                      
                        <i class="fa fa-fw fa-CSDN"></i>CSDN</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#引言"><span class="nav-number">1.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#隐markov过程需要满足的假设条件"><span class="nav-number">2.</span> <span class="nav-text">隐markov过程需要满足的假设条件</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#隐马尔可夫模型的三个基础问题"><span class="nav-number">3.</span> <span class="nav-text">隐马尔可夫模型的三个基础问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#概率计算问题"><span class="nav-number">3.1.</span> <span class="nav-text">概率计算问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#直接计算"><span class="nav-number">3.1.1.</span> <span class="nav-text">直接计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向算法"><span class="nav-number">3.1.2.</span> <span class="nav-text">前向算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#后向算法"><span class="nav-number">3.1.3.</span> <span class="nav-text">后向算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向后向概率表示"><span class="nav-number">3.1.4.</span> <span class="nav-text">前向后向概率表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向后向概率的几个特殊的统计值"><span class="nav-number">3.1.5.</span> <span class="nav-text">前向后向概率的几个特殊的统计值</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参数估计"><span class="nav-number">3.2.</span> <span class="nav-text">参数估计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#监督学习"><span class="nav-number">3.2.1.</span> <span class="nav-text">监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非监督学习"><span class="nav-number">3.2.2.</span> <span class="nav-text">非监督学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#预测问题"><span class="nav-number">3.3.</span> <span class="nav-text">预测问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#近似算法"><span class="nav-number">3.3.1.</span> <span class="nav-text">近似算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Viterbi算法"><span class="nav-number">3.3.2.</span> <span class="nav-text">Viterbi算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#总结"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">changyuan</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">211.1k</span>
  
</div>

<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>


  <!--div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>
-->






<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共211.1k字</span>
</div>
        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    
      <script type="text/javascript">
      function renderGitment(){
        var gitment = new Gitmint({
            id: window.location.pathname, 
            owner: 'changyuanchn',
            repo: 'https://github.com/changyuanchn/changyuanchn.github.io',
            
            lang: "" || navigator.language || navigator.systemLanguage || navigator.userLanguage,
            
            oauth: {
            
            
                client_secret: '82f82fd4b3c71f66c261736a9aca7151809f2c0e',
            
                client_id: 'b260167e7c41f29d0da7'
            }});
        gitment.render('gitment-container');
      }

      
      function showGitment(){
        document.getElementById("gitment-display-button").style.display = "none";
        document.getElementById("gitment-container").style.display = "block";
        renderGitment();
      }
      
      </script>
    







  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
